# üöÄ Configuraci√≥n de Ollama Local

## Paso 1: Descargar e Instalar Ollama

1. **Windows**: Descargar desde https://ollama.ai/download
2. Ejecutar el instalador
3. Reiniciar la computadora

## Paso 2: Descargar un Modelo de IA

Abre **PowerShell o Terminal** y ejecuta UNA de estas opciones:

### Opci√≥n A: Mistral (RECOMENDADO - Balance de velocidad y calidad)
```bash
ollama pull mistral
```
- Tama√±o: ~4.1 GB
- Velocidad: R√°pido (~2-3 seg por respuesta)
- Calidad: Excelente

### Opci√≥n B: Phi (M√ÅS R√ÅPIDO - para PCs lentas)
```bash
ollama pull phi
```
- Tama√±o: ~2.6 GB
- Velocidad: Ultra r√°pido (~1 seg)
- Calidad: Buena

### Opci√≥n C: Neural Chat (MEJOR PARA CONVERSACI√ìN)
```bash
ollama pull neural-chat
```
- Tama√±o: ~4 GB
- Velocidad: Medio (~2 seg)
- Calidad: Muy buena para chat

### Opci√≥n D: Llama2 (M√ÅS POTENTE)
```bash
ollama pull llama2
```
- Tama√±o: ~3.8 GB
- Velocidad: Medio (~3 seg)
- Calidad: Excelente

---

## Paso 3: Ejecutar Ollama

En una terminal nueva, ejecuta:

```bash
ollama serve
```

Deber√≠as ver:
```
2024-02-04 16:00:00 INFO Server listening on 127.0.0.1:11434
```

‚ö†Ô∏è **IMPORTANTE**: Deja esta ventana abierta mientras uses la app.

---

## Paso 4: Probar la App

1. Abre http://localhost:5173
2. Ve a "Sesi√≥n Completa"
3. Haz clic en "Comenzar Sesi√≥n"
4. Habla durante 1-2 minutos
5. Haz clic en "Terminar Grabaci√≥n"
6. **¬°Espera 2-5 segundos** para que Ollama procese
7. Ver√°s el feedback generado por IA

---

## üîß Cambiar de Modelo

Si quieres usar otro modelo, edita esta l√≠nea en `App.jsx` (l√≠nea ~123):

```javascript
body: JSON.stringify({
  model: "mistral",  // ‚Üê Cambia "mistral" por "phi", "neural-chat", etc.
  prompt: prompt,
  stream: false,
  temperature: 0.7,
}),
```

---

## üìä Comparaci√≥n de Modelos

| Modelo | Tama√±o | Velocidad | Calidad | RAM M√≠nima |
|--------|--------|-----------|---------|-----------|
| **phi** | 2.6 GB | ‚ö°‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê | 4 GB |
| **mistral** | 4.1 GB | ‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê | 8 GB |
| **neural-chat** | 4 GB | ‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê | 8 GB |
| **llama2** | 3.8 GB | ‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | 8 GB |

---

## ‚ùì Solucionar Problemas

### Error: "Ollama no est√° disponible"

**Soluci√≥n**:
1. Aseg√∫rate de que ejecutaste `ollama serve` en otra terminal
2. Abre http://localhost:11434 en el navegador - deber√≠a mostrar "Ollama is running"
3. Si no funciona, reinicia Ollama

### Muy lento

**Opciones**:
- Cambia a modelo `phi` (m√°s r√°pido)
- Reduce `temperature` a 0.5 en el c√≥digo
- Cierra otras aplicaciones

### Errores de memoria

**Soluci√≥n**:
- Usa modelo `phi` (m√°s ligero)
- Aumenta RAM disponible
- Cierra navegador y otras apps

---

## üéØ Ventajas de Ollama

‚úÖ **100% Gratis** - Sin costos de API
‚úÖ **Privado** - Los datos no salen tu PC
‚úÖ **R√°pido** - Respuestas locales sin latencia de red
‚úÖ **Sin Internet** - Funciona offline
‚úÖ **Flexible** - Cambia modelos f√°cilmente

---

## üìù Pr√≥ximas Actualizaciones

Puedes ajustar el prompt en `App.jsx` para mejorar la calidad del feedback. Ejemplo:

```javascript
const prompt = `Eres un coach profesional de oratoria especializado en TED talks...
[tu prompt personalizado aqu√≠]`;
```

---

## üöÄ ¬°Listo!

Ya tienes IA local completamente funcional. Disfruta del an√°lisis de oratoria con IA sin costos. üéâ
